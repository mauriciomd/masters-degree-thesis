\chapter{Introdução}
Os avanços nas tecnologias de informação permitiram o armazenamento de grandes e variados conjuntos de dados. Com o advento da \textit{internet}, interações \textit{online} estão cada vez mais presentes no cotidiano, possibilitando a comunicação entre pessoas e empresas de maneira fácil e descomplicada. Como consequência dessa interatividade, a quantidade de dados tem crescido a uma taxa significativa durante os últimos anos \cite{goldschmidt2015data}. 

Os dados, os quais podem ir desde manifestações em redes sociais até movimentações financeiras, são gerados e disponibilizados em diferentes tamanhos e formatos. De acordo com \cite{mcafee2012big}, através desses dados, gerentes podem medir e entender seus negócios e, consequentemente, traduzir esse conhecimento em decisões melhores. 
Porém, a realização das etapas necessárias para extrair conhecimento a partir dos dados implica em altos custos econômicos e computacionais, uma vez que o armazenamento e o processamento dos mesmos não são tarefas triviais. Para \cite{oussous2018big}, essas dificuldades afetam a captura, o armazenamento, a pesquisa, o compartilhamento, a análise, a gerência e a visualização desses dados. Além disso, a segurança e a privacidade são problemas em aplicações orientadas a dados. 

O processamento desses dados requer, por muitas vezes, ferramentas capazes de recorrer ao processamento paralelo e distribuído entre um conjunto de máquinas (\textit{cluster}), além de suportar altas variações no volume de dados utilizado. \textit{Frameworks} baseados no paradigma \textit{MapReduce}, como o Apache Hadoop, têm sido amplamente utilizados para o processamento de grandes volumes de dados. De forma geral, esses \textit{frameworks} oferecem operações de processamento de alto nível e abstrações para acesso aos recursos do \textit{cluster} com o objetivo de facilitar o desenvolvimento de aplicações pelos usuários. 

Entretanto, os \textit{frameworks} baseados no paradigma \textit{MapReduce} falham em oferecer abstrações para acesso à memória distribuída tornando-os ineficientes no processamento de algoritmos de reutilização, como aqueles utilizados em Mineração de Dados e Aprendizado de Máquina, \cite{zaharia2012rdd}. Nesse sentido, o Apache Spark\footnote{Disponível em: https://spark.apache.org/} surge como um \textit{framework} capaz de processar grandes quantidades de dados de maneira paralela e distribuída estendendo o modelo \textit{MapReduce}, já consolidado pelo Apache Hadoop, de modo a facilitar o desenvolvimento de aplicações com reutilização de dados. 

O Spark foi projetado para implementar um mecanismo de execução multiestágio em memória principal juntamente com sua principal abstração, o \textit{Resilient Distributed Datasets} ou simplesmente RDD \cite{zaharia2012rdd}). Dessa forma, o Spark consegue alcançar um desempenho superior quando comparado ao mecanismo baseado em disco utilizado pelo Hadoop. Este desempenho é obtido através da capacidade de processar diversas computações em memória, dispensando a escrita de dados intermediários em disco, diferentemente do Hadoop. Para tanto, o RDD visa encapsular a manipulação dos dados que serão processados pela aplicação, distribuindo-as através dos nodos do \textit{cluster} e assim permitindo a sua execução em memória.

Um RDD consiste em uma coleção imutável de objetos, os quais podem ser operados e processados de forma paralela e distribuída entre os nodos do \textit{cluster}. Uma vez realizado o processamento de um determinado RDD, este pode ser mantido em \textit{cache} para que seja possível reutilizá-lo em futuras computações sem necessidade de efetuar o seu reprocessamento, tornando a execução da aplicação mais eficiente. 

Conforme novos RDDs são armazenados e processados, a memória disponível tende a ficar esgotada e, portanto, políticas de gerenciamento de memória devem ser utilizadas. Assim, em situações de sobrecarga no uso do espaço disponível, o Spark remove partições mantidas em \textit{cache} de acordo com o algoritmo LRU (\textit{Least Recently Used}) \cite{luu2018beginning}, o qual remove partições cujo acesso ocorreu há mais tempo, explorando apenas a localidade temporal do acesso. Desta forma, é possível gerar situações onde apenas uma fração do RDD permanece armazenado em \textit{cache}. 

Entretanto, cabe ao desenvolvedor da aplicação Spark identificar e selecionar os RDDs que deverão ser mantidos em \textit{cache}. De acordo com \cite{zecevic2016spark}, é importante manter os dados em \textit{cache} para algoritmos iterativos, uma vez que esses tendem a reutilizar os dados.  Por padrão, os dados são armazenados em memória principal, uma vez que esta possui uma menor latência para acesso aos dados quando comparado ao armazenamento estável. Entretanto, é possível utilizar o armazenamento estável ou uma combinação deste juntamente com a memória principal.

\section{Justificativa}
Através do RDD, o Spark permite a realização de diversas computações em memória principal a fim de evitar a escrita de resultados intermediários no armazenamento estável. Ainda, RDDs podem ser mantidos em \textit{cache}, possibilitando a sua reutilização na aplicação sem realizar a sua recomputação. Conforme realiza-se a computação dos RDDs da aplicação, a memória tende a ficar sobrecarregada e, portanto, partições devem ser removidas de acordo com o algoritmo LRU.

O LRU é um algoritmo clássico para o gerenciamento de memória, sendo empregado nos mais variados sistemas \cite{robinson1990data}. Este algoritmo é baseado na observação de que partições frequentemente utilizadas em um passado recente tendem a ser acessadas novamente em um futuro próximo. Deste modo, a partição removida da memória é aquela cujo acesso ocorreu há mais tempo, uma vez que esta apresenta a menor chance de ser acessada novamente em um futuro próximo.

Embora o LRU seja de fácil implementação \cite{kim2000low} e apresente bons resultados, há situações onde existe degradação no desempenho. Um exemplo do comportamento indesejado desse algoritmo ocorre quando há acessos cíclicos à memória, como em um \textit{loop}, em que a quantidade de dados manipulados é maior que o espaço disponível em \textit{cache}. Nessas situações,o LRU sempre irá remover um bloco que será acessado em um futuro próximo, uma vez que estes são os blocos acessados há mais tempo \cite{jiang2002lirs}. Deste modo, considera-se apenas o tempo desde o último acesso ao bloco de memória, ignorando a sua necessidade em um futuro bem próximo.

Dentre as classes de problemas abordadas pelo Spark, um dos grandes focos é o processamento de aplicações onde há reutilização de dados em iterações futuras. Especificamente nesta classe de aplicações, o comportamento do acesso aos dados pode ser semelhante ao acesso cíclico à memória, evidenciando um dos problemas da utilização do LRU neste tipo de aplicação. Consequentemente, a utilização deste algoritmo pelo Spark para o gerenciamento da memória pode acarretar em degradação no desempenho do \textit{framework}.

\section{Objetivos do Trabalho}
Buscando alcançar um desempenho superior no processamento de aplicações com reutilização de dados, este trabalho tem como objetivo o desenvolvimento de um modelo de Gerenciamento Dinâmico da Memória no Spark. Por meio deste modelo, busca-se extrair métricas da execução da aplicação e dos RDDs utilizados por esta, de modo a utilizar estas informações para realizar remoção dos dados em \textit{cache}, bem como decidir o momento em que essas operações devem ser executadas.

Para tanto, o modelo de Gerenciamento proposto é dividido em dois componentes: um algoritmo de gerenciamento das partições de RDDs armazenadas em memória e um agente de monitoramento responsável por obter informações sobre a execução de aplicações.

O algoritmo utilizado para realizar o gerenciamento da memória implementada por este modelo visa decidir quais blocos devem ser removidos da memória em situações onde a mesma encontra-se sobrecarregada. Tal implementação é baseada no algoritmo LRU e consiste em combinar a frequência de reutilização, extraída a partir dos RDDs identificados na aplicação, com a localidade temporal proporcionada pelo LRU.

O agente de monitoramento tem como objetivo prever a necessidade de remoção de partições da memória, por meio da instrumentação da execução de aplicações Spark. Este agente consiste em uma aplicação Java, responsável por acompanhar o andamento da execução da aplicação e a quantidade de memória utilizada para realizar o processamento da aplicação em questão.

A previsão da necessidade de liberar espaço em memória é realizada utilizando dois critérios: um \textit{threshold} de ocupação da memória e o estado atual da execução da aplicação. O limiar de ocupação busca definir um limite máximo para a quantidade de memória ocupada, sendo inferior a 100\%, de modo a garantir que haja  espaço disponível para novos dados. O estado atual da execução da aplicação refere-se a quantidade de memória utilizada pela aplicação e a existência de computações pendentes para execução.

Assim, cenários onde a memória ocupada é superior ao \textit{threshold} e ainda há computações pendentes indicam a possível necessidade de liberação de espaço na memória. Uma vez identificada esta necessidade, o agente de monitoramento envia uma notificação ao nodo Spark cuja memória encontra-se sobrecarregada, para que este nodo execute a remoção de partições da memória. Esta notificação é feita através do Apache ZooKeeper.

A validação do modelo para Gerenciamento Dinâmico da Memória é realizada por meio de experimentos, visando avaliar o tempo de execução dos \textit{benchmarks}, visto que o tempo utilizado para processar as aplicações apresenta o maior impacto quando alterada a política de gerenciamento de memória. Através desta avaliação, compara-se o modelo proposta ao algoritmo LRU implementado nativamente pelo Spark em versões 2.x, uma vez que estas versões apresentam diferenças no gerenciamento da memória utilizada. 

Os experimentos foram realizados com \textit{benchmarks} focados na reutilização de dados em computações futuras. Para isto, utilizou-se as aplicações \textit{PageRank}, \textit{Logistic Regression} e \textit{K-Means}, visto que cada aplicação apresenta um comportamento diferente durante sua execução. 

\section{Estrutura do Trabalho}
O restante do trabalho encontra-se organizado da seguinte maneira: no Capítulo \ref{cap:referencial-teorico} são descritos os conceitos fundamentais acerca das ferramentas utilizadas no desenvolvimento do trabalho. Além disso, neste Capítulo é apresentado e discutido os trabalhos relacionados a este, encontrados na literatura. 

O Capítulo \ref{cap:implementacoes} é destinado a descrever o modelo de Gerenciamento Dinâmico proposto por este trabalho. Para tanto, é demonstrando o desenvolvimento do trabalho juntamente com os detalhes de funcionamento e implementação, bem como o processo de experimentação e os resultados obtidos. Por fim, o Capítulo \ref{cap:consideracoes-finais} apresenta as considerações deste trabalho.
