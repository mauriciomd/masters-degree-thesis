\chapter{Considerações Finais} \label{cap:consideracoes-finais}
O processamento em memória no Spark é realizado utilizando sua principal abstração: o RDD. Uma vez computado um RDD, este pode ser armazenado em \textit{cache} na memória principal, a fim de evitar sua recomputação a cada acesso. Em situações de sobrecarga da memória, o Spark utiliza o algoritmo LRU para remover partições de RDDs da memória, o qual considera que uma partição frequentemente acessada no passado tende a ser acessada novamente em um futuro próximo. Entretanto, há situações onde algoritmo LRU pode acarretar em uma degradação no desempenho do sistema, como em situações onde há acesso cíclico à memória e a quantidade de dados é maior que o espaço disponível. Este problema pode ser detectado no Spark durante o processamento de aplicações onde há reuso de dados na aplicação.

Visando otimizar o LRU no processamento de aplicações onde há reuso de dados,  este trabalho teve por objetivo propor um modelo de Gerenciamento Dinâmico de Memória em Aplicações com Reuso de Dados no Apache Spark. O modelo proposto é composto por dois principais componentes, sendo estes (1) uma política de gerenciamento da memória, responsável por gerir as partições de RDDs mantidas em memória e; (2) um agente externo de monitoramento, a fim de acompanhar a execução da aplicação Spark.

A política de gerenciamento implementada consiste em uma otimização do algoritmo LRU, com o objetivo de agregar a localidade temporal juntamente com a Frequência de Reutilização de cada RDD. Para tanto, o algoritmo analisa o grafo de dependências gerado pelo \textit{job} a fim de identificar a frequência em que cada RDD era reutilizado. Já o agente externo visava monitorar a aplicação Spark, a fim de analisar os dados obtidos e prever a necessidade de remoção de partições da memória. Este agente consiste em uma aplicação Java responsável por acompanhar o andamento da execução da aplicação, bem como a quantidade de memória utilizada para realizar o processamento da aplicação Spark em execução. 

A decisão sobre a necessidade de remover partições da memória é realizada utilizando dois critérios: um \textit{threshold} de ocupação da memória e o \textit{status} atual da execução da aplicação. Uma vez identificada a necessidade de realizar a liberação de espaço em memória principal, o agente notifica o nodo cuja memória encontra-se sobrecarregada. Uma vez notificado, o nodo Spark fica responsável por executar a remoção de partições da memória. A notificação do nodo Spark é realizada utilizando o Apache ZooKeeper, onde o Agente de Monitoramento manipula o \textit{znode} associado a cada Spark \textit{Executor} do \textit{cluster}.

O modelo de Gerenciamento Dinâmico foi validado através de experimentos utilizando a plataforma Grid'5000 com os \textit{benchmarks} \textit{PageRank, K-Means e Logistic Regression}. Além disso, foram adotadas as configurações de 1 GB, 1,5 GB, 2 GB, 2,5 GB e 3 GB de memória disponível no Spark. Os experimentos tinham como objetivo analisar o impacto do método de gerenciamento da memória no tempo de execução final da aplicação, comparando o modelo de Gerenciamento Dinâmico implementado pelo trabalho com o algoritmo padrão do Spark, o LRU.

% [corrigido: sem alteracao por enquanto] - Comentario: Não esquecer de considerar o desvio padrão! Com isso, pode até ser maior (ou menor), mas deixar isso claro.
Os resultados obtidos demonstraram que a solução desenvolvida apresenta resultados satisfatórios, podendo alcançar um desempenho superior ao LRU. Em cenários onde é possível prever a necessidade de memória para novos \textit{jobs}, como no \textit{benchmark Logistic Regression}, o modelo de Gerenciamento Dinâmico pode ser até 23,94\% mais rápido que o algoritmo LRU nas mesmas condições. 

Uma característica evidenciada é o melhor aproveitamento da memória, quando utilizada a estratégia de remoção de partições de RDDs de maneira antecipada, implementada pelo modelo de gerenciamento descrito por este trabalho. Esta característica é salientada no \textit{benchmark PageRank} onde o modelo de Gerenciamento Dinâmico reduziu em até 34\% o tempo de execução deste \textit{benchmark} com 1 GB de memória. 

Outra evidência do melhor aproveitamento da memória pode ser encontrada ao utilizar o \textit{benchmark} \textit{Logistic Regression} no cenário com 1,5 GB de memória disponível. Com esta configuração de memória, o modelo de gerenciamento implementado apresentou uma maior estabilidade na execução, possibilitando a execução do \textit{benchmark} na configuração de 1,5 GB devido ao melhor aproveitamento da memória. Além disso, o uso deste modelo proporcionou a redução do tempo médio de execução. Deste modo, o emprego do modelo de Gerenciamento Dinâmico, o qual visa a utilização de métricas obtidas da aplicação em execução para efetuar o gerenciamento da memória do Spark, pode reduzir o tempo médio de execução de aplicações onde há reutilização de dados.

Este trabalho apresentou um modelo de Gerenciamento Dinâmico da Memória utilizado pelo Apache Spark no processamento de aplicações com Reuso de Dados, sendo estas aplicações um dos grandes focos do \textit{framework}. Entretanto, houve situações onde não foi possível determinar a necessidade de remoção de dados da memória de maneira antecipada, uma vez que o \textit{job} da aplicação era composto por apenas um estágio. Deste modo, sugere-se como trabalho futuros a implementação de um sistema de histórico da execução. Através deste histórico, torna-se possível mensurar o fluxo necessário para terminar a execução da aplicação, mesmo que esta aplicação seja composta apenas de estágios com um \textit{job}.

% [corrigido: nada a corrigir aqui] : Comentario: se houve publicacoes
Ademais, um outro grande foco do Spark é o processamento de \textit{streaming} de dados, onde o final do fluxo de dados é desconhecido. Assim, há também a possibilidade do investigação, como trabalhos futuros, do uso do LRU em aplicações onde o tamanho do fluxo de dados é desconhecido. Nestes casos, os requerimentos de memória tornam-se diferentes, uma vez que não é possível saber se há reutilização de dados em um futuro próximo.
